{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "processed_data = np.load('Full_array.npy')\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#The size of the dataset is 10,000; but I only filled the first 9166 with values, the rest are zeros. (shouldn't be counted)\n",
    "number = 9166\n",
    "processed_data = processed_data[0:number]\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to push accuracy to over 90%\n",
    "\n",
    "#idea number 1: run a set of high percentage accuracy classifications through the NN\n",
    "\n",
    "#idea number 2: clean the training set by running through and deleting the bad examples from my data aquisition, were going to \n",
    "#set all arrays of zero to be class A. If I feed this NN nothing, I want it to tell me it is odd.\n",
    "\n",
    "#first lets see what doing number 2 will get us.\n",
    "\n",
    "what = np.array([0,0,1])\n",
    "if what.any() == 0:\n",
    "    print('This works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters:\n",
    "HP_allowed_confidence = 0.85\n",
    "test_train_split = 0.85\n",
    "batch_size = 64\n",
    "epoch_number = 15\n",
    "learning_rate = 0.0025\n",
    "dropout_rate = 0.3\n",
    "CNL1_filters = 32\n",
    "CNL1_kernal_size = 5\n",
    "MPL1_pool_size= (2,2)\n",
    "MPL1_strides = 2\n",
    "CNL2_filters = 64\n",
    "CNL2_kernal_size = 5\n",
    "MPL2_pool_size = (2,2)\n",
    "MPL2_strides = 2\n",
    "\n",
    "#I have a non-round number of examples\n",
    "train_split_indice = int(np.round(test_train_split*number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in my dataset of targets, targets are strings labels under the name \"Class\"\n",
    "galaxyzoo = pd.read_csv(\"zoo2MainSpecz.csv/zoo2MainSpecz.csv\", usecols=[2,3,4,8,15,21,27], nrows=number)\n",
    "#galaxyzoo = pd.read_csv(\"zoo2MainSpecz.csv/zoo2MainSpecz.csv\", nrows=number)\n",
    "Class = galaxyzoo[\"gz2class\"].values\n",
    "RA = galaxyzoo['ra'].values\n",
    "DEC = galaxyzoo['dec'].values\n",
    "Spiral = galaxyzoo['t01_smooth_or_features_a02_features_or_disk_debiased'].values\n",
    "Elliptical = galaxyzoo['t01_smooth_or_features_a01_smooth_debiased'].values\n",
    "Anythingelse = galaxyzoo['t01_smooth_or_features_a03_star_or_artifact_debiased'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to take the first character of the Class string and interpret as a integer, ala MNIST example code\n",
    "dictionary = {'A':int(2),'E':int(1),'S':int(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resave using my dictionary\n",
    "target = np.empty((len(Class)))\n",
    "for i in range(len(Class)):\n",
    "    target[i] = int(dictionary[Class[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through, find the arrays of zero, set that target to 'A' = 2\n",
    "for i in range(len(target)):\n",
    "    if processed_data[i].any() == 0:\n",
    "        target[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split my data between training and test sets\n",
    "train_target = target[0:train_split_indice]\n",
    "test_target = target[train_split_indice:number]\n",
    "train_images = processed_data[0:train_split_indice]\n",
    "test_images = processed_data[train_split_indice:number]\n",
    "\n",
    "np.shape(train_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7791, 28, 28, 1)\n",
      "(865, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#we want to form a subset of our training examples that are probabilities that they are sorted correctly greater than %90\n",
    "#we will then train this dataset additional times.\n",
    "\n",
    "confidence = list()\n",
    "HP_indice = list()\n",
    "for i in range(len(train_images)):\n",
    "    if train_target[i] == 0:\n",
    "        confidence.append(Spherical[i])\n",
    "    if train_target[i] == 1:\n",
    "        confidence.append(Elliptical[i])\n",
    "    if train_target[i] == 2:\n",
    "        confidence.append(Anythingelse[i])\n",
    "    if confidence[i] >= HP_allowed_confidence:\n",
    "        HP_indice.append(i)\n",
    "train_weights = np.asarray(confidence)\n",
    "HP_target = np.empty(len(HP_indice))\n",
    "HP_images = np.empty((len(HP_indice),28,28))\n",
    "for i in range(len(HP_indice)):\n",
    "    HP_target[i] = train_target[HP_indice[i]]\n",
    "    HP_images[i] = train_images[HP_indice[i]]\n",
    "\n",
    "#print(len(HP_indice))\n",
    "HP_images = HP_images.reshape(len(HP_indice),28,28,1)\n",
    "train_images = train_images.reshape(train_split_indice,28,28,1)\n",
    "test_images = test_images.reshape(train_split_indice-number,28,28,1)\n",
    "print(np.shape(train_images))\n",
    "print(np.shape(HP_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define my model, using a CNN with 2 Convolutional layers, 2 max pool layers, 1 dense layer, 1 drop out layer, and another dense layer \n",
    "def create_model(dropout_rate, learning_rate):\n",
    "    \n",
    "    model = keras.Sequential([])\n",
    "    model.add(keras.layers.Conv2D(input_shape=(28,28,1),filters=CNL1_filters,kernel_size=CNL1_kernal_size,padding=\"same\",activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=MPL1_pool_size, strides=MPL1_strides))\n",
    "    model.add(keras.layers.Conv2D(filters=CNL2_filters,kernel_size=CNL2_kernal_size,padding=\"same\",activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=MPL2_pool_size,strides=MPL2_strides))\n",
    "    model.add(keras.layers.Reshape([7*7*64]))\n",
    "    model.add(keras.layers.Dense(units=1024,activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(keras.layers.Dense(units=3,activation=tf.nn.softmax))\n",
    "    adam = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    return(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "865/865 [==============================] - 6s 7ms/step - loss: 0.1277 - acc: 0.9260\n",
      "Epoch 2/2\n",
      "865/865 [==============================] - 4s 4ms/step - loss: 0.0115 - acc: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b5a1a76ef0>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model with the high probability first, i'll guess 5 times, then maybe do 8 next.\n",
    "model=create_model(dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "model.fit(HP_images, HP_target, epochs=2, batch_size=batch_size, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7791/7791 [==============================] - 31s 4ms/step - loss: 0.9704 - acc: 0.4130\n",
      "Epoch 2/15\n",
      "7791/7791 [==============================] - 30s 4ms/step - loss: 0.4771 - acc: 0.4667\n",
      "Epoch 3/15\n",
      "7791/7791 [==============================] - 36s 5ms/step - loss: 0.4655 - acc: 0.5002\n",
      "Epoch 4/15\n",
      "7791/7791 [==============================] - 41s 5ms/step - loss: 0.4560 - acc: 0.5110\n",
      "Epoch 5/15\n",
      "7791/7791 [==============================] - 39s 5ms/step - loss: 0.4479 - acc: 0.5542\n",
      "Epoch 6/15\n",
      "7791/7791 [==============================] - 31s 4ms/step - loss: 0.4280 - acc: 0.5998\n",
      "Epoch 7/15\n",
      "7791/7791 [==============================] - 35s 4ms/step - loss: 0.4224 - acc: 0.6044\n",
      "Epoch 8/15\n",
      "7791/7791 [==============================] - 35s 4ms/step - loss: 0.4158 - acc: 0.6158\n",
      "Epoch 9/15\n",
      "7791/7791 [==============================] - 33s 4ms/step - loss: 0.4094 - acc: 0.6422\n",
      "Epoch 10/15\n",
      "7791/7791 [==============================] - 30s 4ms/step - loss: 0.4034 - acc: 0.6375\n",
      "Epoch 11/15\n",
      "7791/7791 [==============================] - 30s 4ms/step - loss: 0.3967 - acc: 0.6527\n",
      "Epoch 12/15\n",
      "7791/7791 [==============================] - 30s 4ms/step - loss: 0.3874 - acc: 0.6614\n",
      "Epoch 13/15\n",
      "7791/7791 [==============================] - 30s 4ms/step - loss: 0.3800 - acc: 0.6733\n",
      "Epoch 14/15\n",
      "7791/7791 [==============================] - 30s 4ms/step - loss: 0.3821 - acc: 0.6864\n",
      "Epoch 15/15\n",
      "7791/7791 [==============================] - 33s 4ms/step - loss: 0.3813 - acc: 0.6911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b5a1a76c50>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = create_model(dropout_rate=dropout_rate)\n",
    "model.fit(train_images, train_target, epochs=epoch_number, batch_size=batch_size, verbose=1, shuffle=True,\n",
    "         sample_weight = train_weights)\n",
    "#can use sample_weight = train_weights as a method of fit too, but didnt increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1375/1375 [==============================] - 3s 2ms/step\n",
      "Test accuracy: 0.718545454675501\n"
     ]
    }
   ],
   "source": [
    "#model.summary()\n",
    "test_loss, test_acc = model.evaluate(test_images.reshape(number - train_split_indice,28,28,1), test_target)\n",
    "print('Test accuracy:', test_acc)\n",
    "#0.848 fixing it with the zeros fixed\n",
    "#0.850 with the HP dataset implemented\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
