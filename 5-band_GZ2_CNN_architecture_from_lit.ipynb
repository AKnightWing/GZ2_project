{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCNL1_filters = 32\\nCNL1_kernal_size = 5\\nMPL1_pool_size= (2,2)\\nMPL1_strides = 2\\nCNL2_filters = 64\\nCNL2_kernal_size = 5\\nMPL2_pool_size = (2,2)\\nMPL2_strides = 2\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input variables\n",
    "path = 'image_arrays'\n",
    "\n",
    "#model variables\n",
    "test_size = 0.25\n",
    "batch_size = 32\n",
    "epoch_number = 50\n",
    "validation_split = 0.15\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#architecture variables\n",
    "\"\"\"\n",
    "CNL1_filters = 32\n",
    "CNL1_kernal_size = 5\n",
    "MPL1_pool_size= (2,2)\n",
    "MPL1_strides = 2\n",
    "CNL2_filters = 64\n",
    "CNL2_kernal_size = 5\n",
    "MPL2_pool_size = (2,2)\n",
    "MPL2_strides = 2\n",
    "\"\"\"\n",
    "#defined within because were modeling after a paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxyzoo = pd.read_csv(\"zoo2MainSpecz.csv\", usecols=[8], nrows=10000)\n",
    "Class = galaxyzoo[\"gz2class\"].values\n",
    "dictionary = {'A':int(2),'E':int(1),'S':int(0)}\n",
    "#resave using my dictionary\n",
    "target = np.empty((len(Class)))\n",
    "for i in range(len(Class)):\n",
    "    target[i] = int(dictionary[Class[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/preprocessing/image/\n",
    "\n",
    "#datagen flow from directory wont work, need to create a different iterator. Have to combine with the targets somehow. \n",
    "\n",
    "#seems like a tf.data.Dataset is what we want... well, in the future atleast. lets right now get this working by doing something\n",
    "#memory intensive.\n",
    "#def generator(path,batchsize):\n",
    "#    \"\"\"\n",
    "#    I want this function when called provide a \n",
    "#    \"\"\"\n",
    "#    image_list = os.listdir(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7848\n",
      "(7848, 64, 64, 5)\n",
      "7848\n"
     ]
    }
   ],
   "source": [
    "directory = os.listdir(path)\n",
    "data = np.zeros((len(directory),64,64,5))\n",
    "inbetween_target = np.zeros((len(directory)))\n",
    "for i in range(len(directory)):\n",
    "    ith = directory[i].split('_')[-1]\n",
    "    ith = ith.split('.')[0] #we want the str of what element this is so I can grab from my target array the correct label\n",
    "    ith = int(ith)\n",
    "    array = np.load('{}\\\\{}'.format(path,directory[i]))\n",
    "    data[i,:,:,:] = array\n",
    "    inbetween_target[i] = target[ith]\n",
    "target = inbetween_target\n",
    "print(len(target))\n",
    "print(np.shape(data))\n",
    "print(len(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_target, test_target = train_test_split(data, target, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1903.05580.pdf hyperspectral online data augmentation paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    learning_rate = 0.2\\n    if epoch > 10:\\n        learning_rate = 0.02\\n    if epoch > 20:\\n        learning_rate = 0.01\\n    if epoch > 50:\\n        learning_rate = 0.005\\n\\n    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\\n    return learning_rate\\n\\nlr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is pretty neat, you can create a keras callback to display on tensorboard using a simplified summary tf api\n",
    "\n",
    "#and also this is an example of how to change the lr on the fly, which is pretty handy\n",
    "#https://keras.io/callbacks/\n",
    "\n",
    "logdir=\"summaries/scalars/\" + str(datetime.datetime.now().timestamp())\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir,\n",
    "                                                   histogram_freq=1,\n",
    "                                                   write_graph=False,\n",
    "                                                   write_grads=True,)\n",
    "                                                   #write_images=True)\n",
    "\"\"\"\n",
    "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "\"\"\"\n",
    "#Returns a custom learning rate that decreases as epochs progress.\n",
    "\"\"\"\n",
    "    learning_rate = 0.2\n",
    "    if epoch > 10:\n",
    "        learning_rate = 0.02\n",
    "    if epoch > 20:\n",
    "        learning_rate = 0.01\n",
    "    if epoch > 50:\n",
    "        learning_rate = 0.005\n",
    "\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout_rate=dropout_rate, learning_rate=learning_rate):\n",
    "    \n",
    "    model = keras.Sequential([])\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(input_shape=(64,64,5),filters=32,kernel_size=6,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=64,kernel_size=5,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2,))\n",
    "    model.add(keras.layers.Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=128,kernel_size=2,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2,))\n",
    "    model.add(keras.layers.Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.25))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(units=1024,activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(units=64,activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(units=3,activation=tf.nn.softmax))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy',metrics=['accuracy',]) #auroc])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model = create_model(dropout_rate=dropout_rate, learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datagenerator will rotate flip and zoom my data for augmentations, b/c of cosmo principle I must be invarient to these.\n",
    "\n",
    "#interestingly enough, there is no support for datagen right now with 5 channels:\n",
    "#https://github.com/keras-team/keras/issues/4664\n",
    "#that sucks, because if memory was an issue before, now I will have to default on doing my image roatations and zooms before\n",
    "#hand, and that also takes up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        5792      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 33,858,787\n",
      "Trainable params: 33,858,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5003 samples, validate on 883 samples\n",
      "Epoch 1/50\n",
      "5003/5003 [==============================] - 287s 57ms/sample - loss: 0.6831 - acc: 0.6196 - val_loss: 0.5368 - val_acc: 0.7542\n",
      "Epoch 2/50\n",
      "5003/5003 [==============================] - 285s 57ms/sample - loss: 0.5513 - acc: 0.7408 - val_loss: 0.4823 - val_acc: 0.7588\n",
      "Epoch 3/50\n",
      "5003/5003 [==============================] - 294s 59ms/sample - loss: 0.4814 - acc: 0.7883 - val_loss: 0.4507 - val_acc: 0.8086\n",
      "Epoch 4/50\n",
      "5003/5003 [==============================] - 287s 57ms/sample - loss: 0.4431 - acc: 0.8055 - val_loss: 0.4126 - val_acc: 0.8199\n",
      "Epoch 5/50\n",
      "5003/5003 [==============================] - 278s 56ms/sample - loss: 0.3980 - acc: 0.8337 - val_loss: 0.4404 - val_acc: 0.8199\n",
      "Epoch 6/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.3881 - acc: 0.8435 - val_loss: 0.4075 - val_acc: 0.8245\n",
      "Epoch 7/50\n",
      "5003/5003 [==============================] - 280s 56ms/sample - loss: 0.3662 - acc: 0.8419 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 8/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.3427 - acc: 0.8523 - val_loss: 0.4281 - val_acc: 0.8403\n",
      "Epoch 9/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.3238 - acc: 0.8633 - val_loss: 0.4449 - val_acc: 0.8109\n",
      "Epoch 10/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.3062 - acc: 0.8627 - val_loss: 0.4677 - val_acc: 0.8233\n",
      "Epoch 11/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.2822 - acc: 0.8831 - val_loss: 0.5534 - val_acc: 0.8233\n",
      "Epoch 12/50\n",
      "5003/5003 [==============================] - 286s 57ms/sample - loss: 0.2999 - acc: 0.8749 - val_loss: 0.6058 - val_acc: 0.7916\n",
      "Epoch 13/50\n",
      "5003/5003 [==============================] - 282s 56ms/sample - loss: 0.2728 - acc: 0.8905 - val_loss: 0.5124 - val_acc: 0.8165\n",
      "Epoch 14/50\n",
      "5003/5003 [==============================] - 283s 57ms/sample - loss: 0.2318 - acc: 0.9063 - val_loss: 0.5713 - val_acc: 0.7950\n",
      "Epoch 15/50\n",
      "5003/5003 [==============================] - 283s 57ms/sample - loss: 0.2136 - acc: 0.9176 - val_loss: 0.6007 - val_acc: 0.7939\n",
      "Epoch 16/50\n",
      "5003/5003 [==============================] - 278s 56ms/sample - loss: 0.1936 - acc: 0.9272 - val_loss: 0.5869 - val_acc: 0.8131\n",
      "Epoch 17/50\n",
      "5003/5003 [==============================] - 278s 56ms/sample - loss: 0.1867 - acc: 0.9236 - val_loss: 0.6309 - val_acc: 0.8177\n",
      "Epoch 18/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.1798 - acc: 0.9318 - val_loss: 0.6134 - val_acc: 0.7995\n",
      "Epoch 19/50\n",
      "5003/5003 [==============================] - 280s 56ms/sample - loss: 0.1564 - acc: 0.9432 - val_loss: 0.7475 - val_acc: 0.8131\n",
      "Epoch 20/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.1362 - acc: 0.9466 - val_loss: 0.8472 - val_acc: 0.8052\n",
      "Epoch 21/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.1291 - acc: 0.9542 - val_loss: 0.7953 - val_acc: 0.8143\n",
      "Epoch 22/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.1359 - acc: 0.9530 - val_loss: 0.7440 - val_acc: 0.8177\n",
      "Epoch 23/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.0889 - acc: 0.9658 - val_loss: 1.1340 - val_acc: 0.7961\n",
      "Epoch 24/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.0855 - acc: 0.9698 - val_loss: 1.0453 - val_acc: 0.7826\n",
      "Epoch 25/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.0997 - acc: 0.9632 - val_loss: 0.9240 - val_acc: 0.8120\n",
      "Epoch 26/50\n",
      "5003/5003 [==============================] - 280s 56ms/sample - loss: 0.0673 - acc: 0.9770 - val_loss: 1.0386 - val_acc: 0.8131\n",
      "Epoch 27/50\n",
      "5003/5003 [==============================] - 278s 56ms/sample - loss: 0.0592 - acc: 0.9808 - val_loss: 1.1854 - val_acc: 0.7973\n",
      "Epoch 28/50\n",
      "5003/5003 [==============================] - 279s 56ms/sample - loss: 0.0691 - acc: 0.9778 - val_loss: 0.9608 - val_acc: 0.7984\n",
      "Epoch 29/50\n",
      "4992/5003 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9808"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-975cdf0be5a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                     callbacks=[tensorboard_callback])#, lr_callback]) #and will display that on tensorboard too\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m           \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m           validation_in_fit=True)\n\u001b[0m\u001b[0;32m    365\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_target,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epoch_number,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=[tensorboard_callback])#, lr_callback]) #and will display that on tensorboard too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('accuracy-2.png')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('loss-2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_target)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_1\n",
    "\n",
    "test_train_split = 0.75\n",
    "batch_size = 64\n",
    "epoch_number = 20\n",
    "validation_split = 0.15\n",
    "learning_rate = 1e-3\n",
    "dropout_rate = 0.40\n",
    "\n",
    "output: terrible overtraining, but promising that in 20 epochs I hit 92%: reducing learning rate by a lot\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\"\n",
    "model_2\n",
    "\n",
    "test_train_split = 0.75\n",
    "batch_size = 64\n",
    "epoch_number = 20\n",
    "validation_split = 0.15\n",
    "learning_rate = 1e-6\n",
    "dropout_rate = 0.40\n",
    "\n",
    "\n",
    "Test accuracy: 0.60142714\n",
    "Test loss: 0.7398002415622046\n",
    "\n",
    "Epoch 20/20\n",
    "5003/5003 [==============================] - 59s 12ms/sample - loss: 0.6610 - acc: 0.6874 - val_loss: 0.7216 - val_acc: 0.6625\n",
    "\n",
    "Perhaps I was a little overzealous with the LR reduction: I am going to prop it back up, increase dropout.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model_3\n",
    "\n",
    "test_train_split = 0.75\n",
    "batch_size = 64\n",
    "epoch_number = 20\n",
    "validation_split = 0.15\n",
    "learning_rate = 1e-4\n",
    "dropout_rate = 0.45\n",
    "\n",
    "1962/1962 [==============================] - 7s 4ms/sample - loss: 0.9262 - acc: 0.7457\n",
    "Test accuracy: 0.7456677\n",
    "Test loss: 0.9262316330249643\n",
    "\n",
    "Epoch 19/20\n",
    "5003/5003 [==============================] - 58s 12ms/sample - loss: 0.1947 - acc: 0.9310 - val_loss: 0.8856 - val_acc: 0.7610\n",
    "Epoch 20/20\n",
    "5003/5003 [==============================] - 58s 12ms/sample - loss: 0.1838 - acc: 0.9414 - val_loss: 0.9662 - val_acc: 0.7542\n",
    "\n",
    "too high of LR: trying to lower by 1e-1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model_4\n",
    "\n",
    "#model variables\n",
    "test_train_split = 0.75\n",
    "batch_size = 64\n",
    "epoch_number = 20\n",
    "validation_split = 0.15\n",
    "learning_rate = 1e-5\n",
    "dropout_rate = 0.45\n",
    "\n",
    "Epoch 19/20\n",
    "5003/5003 [==============================] - 60s 12ms/sample - loss: 0.4295 - acc: 0.8183 - val_loss: 0.6812 - val_acc: 0.7407\n",
    "Epoch 20/20\n",
    "5003/5003 [==============================] - 60s 12ms/sample - loss: 0.4200 - acc: 0.8279 - val_loss: 0.6655 - val_acc: 0.7293\n",
    "\n",
    "1962/1962 [==============================] - 7s 4ms/sample - loss: 0.6960 - acc: 0.6774\n",
    "Test accuracy: 0.67737\n",
    "Test loss: 0.6960039274286665\n",
    "\n",
    "very weird that this has a lower accuracy, but also much lower loss than model_3, which clearly had too high a LR\n",
    ":Increasing LR slightly, just to see where I can maximize it and still be stable.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model_5\n",
    "\n",
    "test_train_split = 0.75\n",
    "batch_size = 64\n",
    "epoch_number = 20\n",
    "validation_split = 0.15\n",
    "learning_rate = 3e-5\n",
    "dropout_rate = 0.45\n",
    "\n",
    "Epoch 19/20\n",
    "5003/5003 [==============================] - 60s 12ms/sample - loss: 0.3353 - acc: 0.8701 - val_loss: 0.7270 - val_acc: 0.7316\n",
    "Epoch 20/20\n",
    "5003/5003 [==============================] - 60s 12ms/sample - loss: 0.3200 - acc: 0.8811 - val_loss: 0.7252 - val_acc: 0.7395\n",
    "\n",
    "1962/1962 [==============================] - 7s 4ms/sample - loss: 0.7473 - acc: 0.7125\n",
    "Test accuracy: 0.71253824\n",
    "Test loss: 0.747275456770966\n",
    "\n",
    "#I liked it better at lower, and then add more epochs, increase dropout because overfitting is a thing.\n",
    "\n",
    "I forgot to download the graph outputs, but it was basically a straight lines after ~3 epochs along 0.72 accuracy and 0.86 loss\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model_6\n",
    "\n",
    "test_train_split = 0.75\n",
    "batch_size = 64\n",
    "epoch_number = 50\n",
    "validation_split = 0.15\n",
    "learning_rate = 1e-6\n",
    "dropout_rate = 0.50\n",
    "\n",
    "Epoch 49/50\n",
    "5003/5003 [==============================] - 59s 12ms/sample - loss: 0.5778 - acc: 0.7256 - val_loss: 0.6859 - val_acc: 0.6840\n",
    "Epoch 50/50\n",
    "5003/5003 [==============================] - 60s 12ms/sample - loss: 0.5838 - acc: 0.7234 - val_loss: 0.6876 - val_acc: 0.6795\n",
    "\n",
    "\n",
    "1962/1962 [==============================] - 7s 4ms/sample - loss: 0.7073 - acc: 0.6213\n",
    "Test accuracy: 0.6213048\n",
    "Test loss: 0.7072517683503582\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model_7 \n",
    "\n",
    "test_train_split = 0.75\n",
    "batch_size = 64\n",
    "epoch_number = 50\n",
    "validation_split = 0.15\n",
    "learning_rate = 3e-6\n",
    "dropout_rate = 0.50\n",
    "\n",
    "implemented tensorboard during this guy, so can go there to see the runs data and graphs. I think on this next one I want to\n",
    "build a different architecture. There was that paper Gautham posted from 2018 and it woould be neat to use their working arch.\n",
    "But since it is lunch time, I will make a fwe small changed and hit go. I am mainly testing some more tensorboard features on\n",
    "this model. but also, fuck it, I added another dense layer just for shits and giggles.\n",
    "\n",
    "I'm pretty sure since I can get my training accuracy well over 90% (perhaps by overtraining?) that this is unneccessary\n",
    "because this high accuracy means that I am fully able to map the training space. But also-- what the fuck do I know lol?\n",
    "\n",
    "I also increased batchsize apparently.\n",
    "\n",
    "also, auroc isnt working, something abput py.func being deprecated in tf2.0; look that guy up the error said it was a simple\n",
    "fix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all above are models using a different architecture. all below are comments from the current architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source list\n",
    "\"\"\"\n",
    "https://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\n",
    "\n",
    "https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "\n",
    "https://astrobites.org/2018/07/16/creating-a-more-general-deep-learning-algorithm-for-galaxies/\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
