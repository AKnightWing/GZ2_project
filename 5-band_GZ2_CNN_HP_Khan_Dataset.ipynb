{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import random\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.ndimage import shift\n",
    "from skimage.transform import rotate\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input variables\n",
    "target_path = \"C:\\\\Users\\\\awe2\\\\DL_DES-master\\\\data\\\\sdss-galaxyzoo\\\\high_certainty\\\\sdss_metadata\\\\\"\n",
    "\n",
    "#path = 'image_arrays_new_new\\\\'\n",
    "#validation_path = path + 'validation'\n",
    "#training_path = path + 'training'\n",
    "#test_path = path + 'test'\n",
    "\n",
    "#model variables\n",
    "batch_size = 32 #\n",
    "#epoch_number = 50\n",
    "learning_rate = 1e-4 \n",
    "\n",
    "params = {'dim': (120,120),\n",
    "          'batch_size': batch_size,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 5,\n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "#more parameters means more prone to overfitting, and I am 5/3 times worse on parameters compared to the paper I have\n",
    "#based this on. (5 bands instead of 3) I need to find ways to add more regularization, or otherwise might try reducing my number\n",
    "#of layers to reduce the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/37119071/scipy-rotate-and-zoom-an-image-without-changing-its-dimensions/48097478\n",
    "def clipped_zoom(img, zoom_factor, **kwargs):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # For multichannel images we don't want to apply the zoom factor to the RGB\n",
    "    # dimension, so instead we create a tuple of zoom factors, one per array\n",
    "    # dimension, with 1's for any trailing dimensions after the width and height.\n",
    "    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n",
    "\n",
    "    # Zooming out\n",
    "    if zoom_factor < 1:\n",
    "\n",
    "        # Bounding box of the zoomed-out image within the output array\n",
    "        zh = int(np.round(h * zoom_factor))\n",
    "        zw = int(np.round(w * zoom_factor))\n",
    "        top = (h - zh) // 2\n",
    "        left = (w - zw) // 2\n",
    "\n",
    "        # Zero-padding\n",
    "        out = np.zeros_like(img)\n",
    "        out[top:top+zh, left:left+zw] = zoom(img, zoom_tuple, **kwargs)\n",
    "\n",
    "    # Zooming in\n",
    "    elif zoom_factor > 1:\n",
    "\n",
    "        # Bounding box of the zoomed-in region within the input array\n",
    "        zh = int(np.round(h / zoom_factor))\n",
    "        zw = int(np.round(w / zoom_factor))\n",
    "        top = (h - zh) // 2\n",
    "        left = (w - zw) // 2\n",
    "\n",
    "        out = zoom(img[top:top+zh, left:left+zw], zoom_tuple, **kwargs)\n",
    "\n",
    "        # `out` might still be slightly larger than `img` due to rounding, so\n",
    "        # trim off any extra pixels at the edges\n",
    "        trim_top = ((out.shape[0] - h) // 2)\n",
    "        trim_left = ((out.shape[1] - w) // 2)\n",
    "        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n",
    "\n",
    "    # If zoom_factor == 1, just return the input array\n",
    "    else:\n",
    "        out = img\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(120,120), n_channels=3,\n",
    "                 n_classes=2, shuffle=True):\n",
    "     #   'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "    #'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "    #'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "    # Initialization\n",
    "    \n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "    \n",
    "\n",
    "      # Generate data and perform augmentation\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "          # Store sample\n",
    "            X[i,] = np.load('HP_inputs/' + ID + '.npy')\n",
    "            \n",
    "            #I messed up when generating my inputs on the order of the band; I want to train the model in the usual bands:\n",
    "            \n",
    "            #u_filter = X[i,:,:,0]\n",
    "            #g_filter = X[i,:,:,1]\n",
    "            i_filter = X[i,:,:,2]\n",
    "            r_filter = X[i,:,:,3]\n",
    "            #z_filter = X[i,:,:,4]\n",
    "\n",
    "            #X[i,:,:,0] = u_filter\n",
    "            #X[i,:,:,1] = g_filter\n",
    "            X[i,:,:,2] = r_filter\n",
    "            X[i,:,:,3] = i_filter\n",
    "            #X[i,:,:,4] = z_filter              \n",
    "            #flip\n",
    "            if random.random() > 0.5:\n",
    "                X[i,] = np.flip(X[i,],0)\n",
    "            if random.random() > 0.5:\n",
    "                X[i,] = np.flip(X[i,],1)\n",
    "            \n",
    "            #shift\n",
    "            if random.random() > 0.5 :\n",
    "                X[i,] = shift(X[i,], (4,0,0), mode='nearest')\n",
    "            elif random.random() > 0.5 :\n",
    "                X[i,] = shift(X[i,], (-4,0,0), mode='nearest')\n",
    "                              \n",
    "            if random.random() > 0.5 :\n",
    "                X[i,] = shift(X[i,], (0,4,0), mode='nearest')\n",
    "            elif random.random() > 0.5 :\n",
    "                X[i,] = shift(X[i,], (0,-4,0), mode='nearest')\n",
    "          \n",
    "            #zoom in/out\n",
    "            zoom_factor = random.uniform(0.75,1.3)\n",
    "            X[i,] = clipped_zoom(X[i,],zoom_factor)\n",
    "            \n",
    "            #rotate\n",
    "            angle = 45*random.random()\n",
    "            X[i,] = rotate(X[i,], angle=angle, mode='reflect')\n",
    "            \n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "    \n",
    "        if self.n_classes > 2:\n",
    "            return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "    #'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    #  'Generate one batch of data'\n",
    "      # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "      # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "      # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the targets.\n",
    "a = []\n",
    "for file in (os.listdir(target_path)):\n",
    "    target = pd.read_csv(target_path+file,usecols=[14])\n",
    "    a.append((target[\"target\"].values))\n",
    "targets = np.concatenate([a[0],a[1],a[2],a[3],a[4],a[5],a[6],a[7],a[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targets each correspond to an array called: HP_*****.npy in folder HP_inputs// where ***** is zfilled starting at 00000\n",
    "#we can use os.listdir to generate strings of the correct iterator\n",
    "\n",
    "#we then need to sort out where each of these files will go.\n",
    "#for i,file in enumerate(os.listdir(\"HP_inputs\"))\n",
    "\n",
    "\"\"\"\n",
    "labels = {}\n",
    "for i in range(len(targets)):\n",
    "    string = \"HP_{}.npy\".format(str(i).zfill(5))\n",
    "    if string in os.listdir(\"HP_inputs\"):\n",
    "        name = \"HP_{}\".format(str(i).zfill(5))\n",
    "        labels.update({name:targets[i]})\n",
    "        \n",
    "f = open(\"HP_inputs\\\\labels.txt\",\"w\")\n",
    "f.write( str(labels) )\n",
    "f.close()\n",
    "\"\"\"\n",
    "with open(\"labels.txt\",\"r\") as inf:\n",
    "    labels = eval(inf.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we shuffle our data list, and partition into train, test, and validation sets.\n",
    "data_list = os.listdir(\"HP_inputs\")\n",
    "random.shuffle(data_list)\n",
    "#we want 20% of the data reserved for testing, and then another 15% reserved for validation.\n",
    "top_index = len(data_list)\n",
    "first_index = int(round(top_index * 0.65))\n",
    "second_index = int(round(top_index * 0.15)) + first_index\n",
    "\n",
    "train_list = data_list[0:first_index]\n",
    "val_list = data_list[first_index:second_index]\n",
    "test_list = data_list[second_index::]\n",
    "\n",
    "for i,file in enumerate(train_list):\n",
    "    train_list[i] = file.split('.')[0]\n",
    "for i,file in enumerate(val_list):\n",
    "    val_list[i] = file.split('.')[0]\n",
    "for i,file in enumerate(test_list):\n",
    "    test_list[i] = file.split('.')[0]\n",
    "\n",
    "partition = {'train':train_list,'validation':val_list,'test':test_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
    "test_generator = DataGenerator(partition['test'], labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so this is pretty neat, you can create a keras callback to display on tensorboard using a simplified summary tf api\n",
    "\n",
    "#and also this is an example of how to change the lr on the fly, which is pretty handy\n",
    "#https://keras.io/callbacks/\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "    file_writer.set_as_default()\n",
    "\"\"\"\n",
    "def lr_schedule(epoch,lr):\n",
    "\n",
    "#Returns a custom learning rate that decreases as epochs progress.\n",
    "    if epoch > 1:\n",
    "        lr = 1e-4\n",
    "    if epoch > 7:\n",
    "        lr = 1e-5\n",
    "    if epoch > 12:\n",
    "        lr = 1e-6\n",
    "    if epoch > 20:\n",
    "        lr = 5e-6\n",
    "\n",
    "    tf.summary.scalar('learning_rate', tensor=lr)\n",
    "    return lr\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "#logdir=\"summaries/scalars/\" + str(datetime.datetime.now().timestamp())\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir,\n",
    "#                                                   histogram_freq=1,\n",
    "#                                                   write_graph=False,\n",
    "#                                                   write_grads=True,)\n",
    "#                                                   #write_images=True)\n",
    "#will it still print stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate=learning_rate):\n",
    "    \n",
    "    model = keras.Sequential([])\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(input_shape=(120,120,5),filters=32,kernel_size=6,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=64,kernel_size=5,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2,))\n",
    "    model.add(keras.layers.Dropout(rate=0.25)) #best = 0.25\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=128,kernel_size=2,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2,))\n",
    "    model.add(keras.layers.Dropout(rate=0.25)) #best = 0.25\n",
    "    \n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.35)) #best = 0.35\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(units=64,activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.5))\n",
    "    model.add(keras.layers.Dense(units=1,activation=tf.nn.sigmoid)) #tf.nn.softmax for categorical, sigmoid for binary\n",
    "    \n",
    "    adam = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy',metrics=['accuracy',auc]) \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model = create_model(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785\n",
      "181\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "steps_to_take = int(len(train_list)/batch_size)\n",
    "val_steps_to_take = int(len(val_list)/batch_size)\n",
    "test_steps_to_take = int(len(test_list)/batch_size)\n",
    "                #typically be equal to the number of unique samples if your dataset\n",
    "                #divided by the batch size.\n",
    "\n",
    "print(steps_to_take)\n",
    "print(val_steps_to_take)\n",
    "print(test_steps_to_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another callback\n",
    "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-8, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"models//5-band-CNN-correct-filters.hdf5\"\n",
    "ModelCheckpointCB = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\interpolation.py:605: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784/785 [============================>.] - ETA: 2s - loss: 1.0354 - acc: 0.5412\n",
      "Epoch 00001: val_loss improved from inf to 0.64114, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2708s 3s/step - loss: 1.0349 - acc: 0.5414 - val_loss: 0.6411 - val_acc: 0.5478\n",
      "Epoch 2/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.6090 - acc: 0.6707\n",
      "Epoch 00002: val_loss improved from 0.64114 to 0.55401, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2589s 3s/step - loss: 0.6090 - acc: 0.6707 - val_loss: 0.5540 - val_acc: 0.7623\n",
      "Epoch 3/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.5782 - acc: 0.6941\n",
      "Epoch 00003: val_loss improved from 0.55401 to 0.52204, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2579s 3s/step - loss: 0.5782 - acc: 0.6942 - val_loss: 0.5220 - val_acc: 0.8565\n",
      "Epoch 4/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.5692 - acc: 0.6979\n",
      "Epoch 00004: val_loss improved from 0.52204 to 0.49604, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2572s 3s/step - loss: 0.5692 - acc: 0.6979 - val_loss: 0.4960 - val_acc: 0.8950\n",
      "Epoch 5/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.5563 - acc: 0.7058\n",
      "Epoch 00005: val_loss improved from 0.49604 to 0.46125, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2580s 3s/step - loss: 0.5562 - acc: 0.7057 - val_loss: 0.4612 - val_acc: 0.9239\n",
      "Epoch 6/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.5532 - acc: 0.7050\n",
      "Epoch 00006: val_loss improved from 0.46125 to 0.45193, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2683s 3s/step - loss: 0.5531 - acc: 0.7051 - val_loss: 0.4519 - val_acc: 0.9258\n",
      "Epoch 7/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.5274 - acc: 0.7345\n",
      "Epoch 00007: val_loss improved from 0.45193 to 0.39570, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2607s 3s/step - loss: 0.5273 - acc: 0.7345 - val_loss: 0.3957 - val_acc: 0.9380\n",
      "Epoch 8/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.4589 - acc: 0.8204\n",
      "Epoch 00008: val_loss improved from 0.39570 to 0.36737, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2567s 3s/step - loss: 0.4590 - acc: 0.8203 - val_loss: 0.3674 - val_acc: 0.9506\n",
      "Epoch 9/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.3664 - acc: 0.8317\n",
      "Epoch 00009: val_loss improved from 0.36737 to 0.20604, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2566s 3s/step - loss: 0.3663 - acc: 0.8317 - val_loss: 0.2060 - val_acc: 0.9501\n",
      "Epoch 10/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.3290 - acc: 0.8315\n",
      "Epoch 00010: val_loss improved from 0.20604 to 0.15185, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2576s 3s/step - loss: 0.3288 - acc: 0.8315 - val_loss: 0.1519 - val_acc: 0.9515\n",
      "Epoch 11/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2839 - acc: 0.8337\n",
      "Epoch 00011: val_loss improved from 0.15185 to 0.13479, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2571s 3s/step - loss: 0.2840 - acc: 0.8335 - val_loss: 0.1348 - val_acc: 0.9560\n",
      "Epoch 12/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2798 - acc: 0.8369\n",
      "Epoch 00012: val_loss did not improve from 0.13479\n",
      "785/785 [==============================] - 2576s 3s/step - loss: 0.2799 - acc: 0.8368 - val_loss: 0.1422 - val_acc: 0.9574\n",
      "Epoch 13/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2724 - acc: 0.8415\n",
      "Epoch 00013: val_loss did not improve from 0.13479\n",
      "785/785 [==============================] - 2565s 3s/step - loss: 0.2724 - acc: 0.8416 - val_loss: 0.1393 - val_acc: 0.9508\n",
      "Epoch 14/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2403 - acc: 0.8822\n",
      "Epoch 00014: val_loss improved from 0.13479 to 0.12524, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2578s 3s/step - loss: 0.2403 - acc: 0.8822 - val_loss: 0.1252 - val_acc: 0.9556\n",
      "Epoch 15/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2454 - acc: 0.8812\n",
      "Epoch 00015: val_loss improved from 0.12524 to 0.12445, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2578s 3s/step - loss: 0.2456 - acc: 0.8811 - val_loss: 0.1244 - val_acc: 0.9582\n",
      "Epoch 16/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2369 - acc: 0.8861\n",
      "Epoch 00016: val_loss improved from 0.12445 to 0.12024, saving model to models//5-band-CNN-correct-filters.hdf5\n",
      "785/785 [==============================] - 2582s 3s/step - loss: 0.2368 - acc: 0.8861 - val_loss: 0.1202 - val_acc: 0.9591\n",
      "Epoch 17/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2359 - acc: 0.8872\n",
      "Epoch 00017: val_loss did not improve from 0.12024\n",
      "785/785 [==============================] - 2647s 3s/step - loss: 0.2359 - acc: 0.8872 - val_loss: 0.1228 - val_acc: 0.9579\n",
      "Epoch 18/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2367 - acc: 0.8885\n",
      "Epoch 00018: val_loss did not improve from 0.12024\n",
      "785/785 [==============================] - 2614s 3s/step - loss: 0.2368 - acc: 0.8884 - val_loss: 0.1242 - val_acc: 0.9537\n",
      "Epoch 19/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2353 - acc: 0.8860\n",
      "Epoch 00019: val_loss did not improve from 0.12024\n",
      "785/785 [==============================] - 2560s 3s/step - loss: 0.2353 - acc: 0.8861 - val_loss: 0.1251 - val_acc: 0.9536\n",
      "Epoch 20/20\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2327 - acc: 0.8875\n",
      "Epoch 00020: val_loss did not improve from 0.12024\n",
      "785/785 [==============================] - 2574s 3s/step - loss: 0.2328 - acc: 0.8874 - val_loss: 0.1246 - val_acc: 0.9561\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(generator=training_generator,\n",
    "                    steps_per_epoch=steps_to_take, \n",
    "                    epochs=20,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=val_steps_to_take,\n",
    "                    verbose=1,\n",
    "                    callbacks=[ModelCheckpointCB,lr_callback])\n",
    "                    #callbacks=[tensorboard_callback,lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loss, test_acc = model.evaluate(test_images, test_target)\n",
    "#print('Test accuracy:', test_acc)\n",
    "#print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"models//5-band-CNN-correct-filters-little-more.hdf5\"\n",
    "ModelCheckpointCB = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2327 - acc: 0.8889\n",
      "Epoch 00022: val_loss improved from inf to 0.12059, saving model to models//5-band-CNN-correct-filters-little-more.hdf5\n",
      "785/785 [==============================] - 2611s 3s/step - loss: 0.2325 - acc: 0.8890 - val_loss: 0.1206 - val_acc: 0.9574\n",
      "Epoch 23/25\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2243 - acc: 0.8927\n",
      "Epoch 00023: val_loss improved from 0.12059 to 0.10971, saving model to models//5-band-CNN-correct-filters-little-more.hdf5\n",
      "785/785 [==============================] - 2575s 3s/step - loss: 0.2243 - acc: 0.8927 - val_loss: 0.1097 - val_acc: 0.9598\n",
      "Epoch 24/25\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2218 - acc: 0.8949\n",
      "Epoch 00024: val_loss did not improve from 0.10971\n",
      "785/785 [==============================] - 2623s 3s/step - loss: 0.2219 - acc: 0.8949 - val_loss: 0.1215 - val_acc: 0.9553\n",
      "Epoch 25/25\n",
      "784/785 [============================>.] - ETA: 2s - loss: 0.2208 - acc: 0.9053\n",
      "Epoch 00025: val_loss did not improve from 0.10971\n",
      "785/785 [==============================] - 2647s 3s/step - loss: 0.2209 - acc: 0.9052 - val_loss: 0.1223 - val_acc: 0.9560\n"
     ]
    }
   ],
   "source": [
    "hist_1 = model.fit_generator(generator=training_generator,\n",
    "                    steps_per_epoch=steps_to_take, \n",
    "                    epochs=25,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=val_steps_to_take,\n",
    "                    verbose=1,\n",
    "                    initial_epoch=21,\n",
    "                    callbacks=[ModelCheckpointCB,lr_callback])\n",
    "                    #callbacks=[tensorboard_callback,lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_prob = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\\n\\nhttps://arxiv.org/pdf/1711.05744.pdf\\n\\nhttps://arxiv.org/pdf/1807.00807.pdf\\n\\nhttps://github.com/jameslawlor/kaggle_galaxy_zoo/blob/master/galaxy_zoo_keras.ipynb\\n\\nhttps://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\\n\\n#https://stackoverflow.com/questions/37119071/scipy-rotate-and-zoom-an-image-without-changing-its-dimensions/48097478\\n\\nhttps://distill.pub/2018/building-blocks/ what I want to do with this after it is working.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source list\n",
    "\"\"\"\n",
    "https://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\n",
    "\n",
    "https://arxiv.org/pdf/1711.05744.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1807.00807.pdf\n",
    "\n",
    "https://github.com/jameslawlor/kaggle_galaxy_zoo/blob/master/galaxy_zoo_keras.ipynb\n",
    "\n",
    "https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "#https://stackoverflow.com/questions/37119071/scipy-rotate-and-zoom-an-image-without-changing-its-dimensions/48097478\n",
    "\n",
    "https://distill.pub/2018/building-blocks/ what I want to do with this after it is working.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n784/785 [============================>.] - ETA: 3s - loss: 1.6864 - acc: 0.5130\\nEpoch 00001: val_loss improved from inf to 0.68030, saving model to models//5-band-CNN\\n785/785 [==============================] - 3110s 4s/step - loss: 1.6851 - acc: 0.5130 - val_loss: 0.6803 - val_acc: 0.5537\\nEpoch 2/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.6738 - acc: 0.5632\\nEpoch 00002: val_loss improved from 0.68030 to 0.65193, saving model to models//5-band-CNN\\n785/785 [==============================] - 2741s 3s/step - loss: 0.6738 - acc: 0.5632 - val_loss: 0.6519 - val_acc: 0.5730\\nEpoch 3/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.6481 - acc: 0.5953\\nEpoch 00003: val_loss improved from 0.65193 to 0.65075, saving model to models//5-band-CNN\\n785/785 [==============================] - 2677s 3s/step - loss: 0.6480 - acc: 0.5953 - val_loss: 0.6507 - val_acc: 0.6143\\nEpoch 4/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.6387 - acc: 0.6149\\nEpoch 00004: val_loss did not improve from 0.65075\\n785/785 [==============================] - 2680s 3s/step - loss: 0.6387 - acc: 0.6149 - val_loss: 0.7353 - val_acc: 0.6195\\nEpoch 5/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.5721 - acc: 0.6687\\nEpoch 00005: val_loss improved from 0.65075 to 0.39564, saving model to models//5-band-CNN\\n785/785 [==============================] - 2673s 3s/step - loss: 0.5718 - acc: 0.6689 - val_loss: 0.3956 - val_acc: 0.8078\\nEpoch 6/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.2843 - acc: 0.8756\\nEpoch 00006: val_loss did not improve from 0.39564\\n785/785 [==============================] - 2667s 3s/step - loss: 0.2842 - acc: 0.8756 - val_loss: 0.4010 - val_acc: 0.8995\\nEpoch 7/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.2060 - acc: 0.9138\\nEpoch 00007: val_loss did not improve from 0.39564\\n785/785 [==============================] - 2677s 3s/step - loss: 0.2062 - acc: 0.9138 - val_loss: 0.4315 - val_acc: 0.8881\\nEpoch 8/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.1737 - acc: 0.9292\\nEpoch 00008: val_loss improved from 0.39564 to 0.24017, saving model to models//5-band-CNN\\n785/785 [==============================] - 2675s 3s/step - loss: 0.1738 - acc: 0.9292 - val_loss: 0.2402 - val_acc: 0.9069\\nEpoch 9/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.1621 - acc: 0.9383\\nEpoch 00009: val_loss did not improve from 0.24017\\n785/785 [==============================] - 2678s 3s/step - loss: 0.1620 - acc: 0.9383 - val_loss: 0.3230 - val_acc: 0.9297\\nEpoch 10/10\\n784/785 [============================>.] - ETA: 2s - loss: 0.1451 - acc: 0.9423\\nEpoch 00010: val_loss did not improve from 0.24017\\n785/785 [==============================] - 2667s 3s/step - loss: 0.1451 - acc: 0.9422 - val_loss: 0.2926 - val_acc: 0.9496\\n\\nIntesting, it seems I am not very stable. I want to run this a few more epochs then throw on xception.\\n\\n#after running for a few more epochs, I have decided to re-start. it seems after ~13 epochs the model loses all stability. \\n#begins increasing dramatically. I am lowering starting LR and placing in a LR scheduler.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#model died, I need to implement a save model callback\n",
    "\n",
    "785/785 [==============================] - 2908s 4s/step - loss: 1.3123 - acc: 0.7658 - val_loss: 0.3576 - val_acc: 0.8862\n",
    "Epoch 2/15\n",
    "785/785 [==============================] - 2804s 4s/step - loss: 0.2477 - acc: 0.8998 - val_loss: 0.2727 - val_acc: 0.9176\n",
    "Epoch 3/15\n",
    "785/785 [==============================] - 2730s 3s/step - loss: 0.2065 - acc: 0.9165 - val_loss: 0.2536 - val_acc: 0.9209\n",
    "Epoch 4/15\n",
    "785/785 [==============================] - 2701s 3s/step - loss: 0.1747 - acc: 0.9301 - val_loss: 0.3375 - val_acc: 0.8857\n",
    "Epoch 5/15\n",
    "574/785 [====================>.........] - ETA: 10:50 - loss: 0.1768 - acc: 0.9268\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "784/785 [============================>.] - ETA: 3s - loss: 1.6864 - acc: 0.5130\n",
    "Epoch 00001: val_loss improved from inf to 0.68030, saving model to models//5-band-CNN\n",
    "785/785 [==============================] - 3110s 4s/step - loss: 1.6851 - acc: 0.5130 - val_loss: 0.6803 - val_acc: 0.5537\n",
    "Epoch 2/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.6738 - acc: 0.5632\n",
    "Epoch 00002: val_loss improved from 0.68030 to 0.65193, saving model to models//5-band-CNN\n",
    "785/785 [==============================] - 2741s 3s/step - loss: 0.6738 - acc: 0.5632 - val_loss: 0.6519 - val_acc: 0.5730\n",
    "Epoch 3/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.6481 - acc: 0.5953\n",
    "Epoch 00003: val_loss improved from 0.65193 to 0.65075, saving model to models//5-band-CNN\n",
    "785/785 [==============================] - 2677s 3s/step - loss: 0.6480 - acc: 0.5953 - val_loss: 0.6507 - val_acc: 0.6143\n",
    "Epoch 4/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.6387 - acc: 0.6149\n",
    "Epoch 00004: val_loss did not improve from 0.65075\n",
    "785/785 [==============================] - 2680s 3s/step - loss: 0.6387 - acc: 0.6149 - val_loss: 0.7353 - val_acc: 0.6195\n",
    "Epoch 5/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.5721 - acc: 0.6687\n",
    "Epoch 00005: val_loss improved from 0.65075 to 0.39564, saving model to models//5-band-CNN\n",
    "785/785 [==============================] - 2673s 3s/step - loss: 0.5718 - acc: 0.6689 - val_loss: 0.3956 - val_acc: 0.8078\n",
    "Epoch 6/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.2843 - acc: 0.8756\n",
    "Epoch 00006: val_loss did not improve from 0.39564\n",
    "785/785 [==============================] - 2667s 3s/step - loss: 0.2842 - acc: 0.8756 - val_loss: 0.4010 - val_acc: 0.8995\n",
    "Epoch 7/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.2060 - acc: 0.9138\n",
    "Epoch 00007: val_loss did not improve from 0.39564\n",
    "785/785 [==============================] - 2677s 3s/step - loss: 0.2062 - acc: 0.9138 - val_loss: 0.4315 - val_acc: 0.8881\n",
    "Epoch 8/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.1737 - acc: 0.9292\n",
    "Epoch 00008: val_loss improved from 0.39564 to 0.24017, saving model to models//5-band-CNN\n",
    "785/785 [==============================] - 2675s 3s/step - loss: 0.1738 - acc: 0.9292 - val_loss: 0.2402 - val_acc: 0.9069\n",
    "Epoch 9/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.1621 - acc: 0.9383\n",
    "Epoch 00009: val_loss did not improve from 0.24017\n",
    "785/785 [==============================] - 2678s 3s/step - loss: 0.1620 - acc: 0.9383 - val_loss: 0.3230 - val_acc: 0.9297\n",
    "Epoch 10/10\n",
    "784/785 [============================>.] - ETA: 2s - loss: 0.1451 - acc: 0.9423\n",
    "Epoch 00010: val_loss did not improve from 0.24017\n",
    "785/785 [==============================] - 2667s 3s/step - loss: 0.1451 - acc: 0.9422 - val_loss: 0.2926 - val_acc: 0.9496\n",
    "\n",
    "Intesting, it seems I am not very stable. I want to run this a few more epochs then throw on xception.\n",
    "\n",
    "#after running for a few more epochs, I have decided to re-start. it seems after ~13 epochs the model loses all stability. \n",
    "#begins increasing dramatically. I am lowering starting LR and placing in a LR scheduler.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is how to load in a model's weights\n",
    "\n",
    "#model_new = create_model()\n",
    "#model_new.load_weights(\"models//5-band-CNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\interpolation.py:605: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 664s 3s/step - loss: 0.1155 - acc: 0.9593 - auc: 0.9947\n"
     ]
    }
   ],
   "source": [
    "#test model performance:\n",
    "model.load_weights(\"models//5-band-CNN-correct-filters-little-more.hdf5\")\n",
    "\n",
    "hist2 = model.evaluate_generator(generator=test_generator,\n",
    "                    steps=test_steps_to_take, \n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(hist2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
