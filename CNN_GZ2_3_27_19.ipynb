{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "processed_data = np.load('Full_array.npy')\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#The size of the dataset is 10,000; but I only filled the first 9166 with values, the rest are zeros. (shouldn't be counted)\n",
    "number = 9166\n",
    "processed_data = processed_data[0:number]\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to push accuracy to over 90%\n",
    "\n",
    "#idea number 1: run a set of high percentage accuracy classifications through the NN (nope)\n",
    "\n",
    "#idea number 2: clean the training set by running through and deleting the bad examples from my data aquisition, were going to \n",
    "#set all arrays of zero to be class A. If I feed this NN nothing, I want it to tell me it is odd. (yup)\n",
    "\n",
    "#first lets see what doing number 2 will get us.\n",
    "\n",
    "#augment data set manually (worked)\n",
    "\n",
    "#change LR (Adam is adaptive: so no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to normalize our iamge data to go from 0 to 1\n",
    "#processed_data = processed_data/255.\n",
    "#I actually am not sure how it is scaled. however, some of the pixels are negative for some reason???\n",
    "\n",
    "#also, this did not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters:\n",
    "HP_allowed_confidence = 0.98\n",
    "test_train_split = 0.85\n",
    "batch_size = 128\n",
    "epoch_number = 20\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.25\n",
    "CNL1_filters = 32\n",
    "CNL1_kernal_size = 5\n",
    "MPL1_pool_size= (2,2)\n",
    "MPL1_strides = 2\n",
    "CNL2_filters = 64\n",
    "CNL2_kernal_size = 5\n",
    "MPL2_pool_size = (2,2)\n",
    "MPL2_strides = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in my dataset of targets, targets are strings labels under the name \"Class\"\n",
    "galaxyzoo = pd.read_csv(\"zoo2MainSpecz.csv/zoo2MainSpecz.csv\", usecols=[2,3,4,8,15,21,27], nrows=number)\n",
    "#galaxyzoo = pd.read_csv(\"zoo2MainSpecz.csv/zoo2MainSpecz.csv\", nrows=number)\n",
    "Class = galaxyzoo[\"gz2class\"].values\n",
    "RA = galaxyzoo['ra'].values\n",
    "DEC = galaxyzoo['dec'].values\n",
    "Spiral = galaxyzoo['t01_smooth_or_features_a02_features_or_disk_debiased'].values\n",
    "Elliptical = galaxyzoo['t01_smooth_or_features_a01_smooth_debiased'].values\n",
    "Anythingelse = galaxyzoo['t01_smooth_or_features_a03_star_or_artifact_debiased'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to take the first character of the Class string and interpret as a integer, ala MNIST example code\n",
    "dictionary = {'A':int(2),'E':int(1),'S':int(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resave using my dictionary\n",
    "target = np.empty((len(Class)))\n",
    "for i in range(len(Class)):\n",
    "    target[i] = int(dictionary[Class[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through, find the arrays of zero, set that target to 'A' = 2\n",
    "for i in range(len(target)):\n",
    "    if processed_data[i].any() == 0:\n",
    "        target[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18332, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "#rotate each image in the target_images by a random amount\n",
    "rotations_array = np.empty((np.shape(processed_data)[0],28,28))\n",
    "for i in range(np.shape(processed_data)[0]):\n",
    "    degree = np.random.uniform(-180,180,1)\n",
    "    img = Image.fromarray(processed_data[i])\n",
    "    rot = img.rotate(degree)\n",
    "    rot = np.asarray(rot)\n",
    "    rotations_array[i] = rot\n",
    "#now append rotations_array to train_images\n",
    "rotated_images = np.append(processed_data, rotations_array,0)\n",
    "#now append the class labels to the train_target array\n",
    "rotated_target = np.append(target,target)\n",
    "print(np.shape(rotated_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36664, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "rotations_array = np.empty((np.shape(rotated_images)[0],28,28))\n",
    "for i in range(np.shape(processed_data)[0]):\n",
    "    degree = np.random.uniform(-180,180,1)\n",
    "    img = Image.fromarray(processed_data[i])\n",
    "    rot = img.rotate(degree)\n",
    "    rot = np.asarray(rot)\n",
    "    rotations_array[i] = rot\n",
    "#now append rotations_array to train_images\n",
    "rotated_images = np.append(rotated_images, rotations_array,0)\n",
    "#now append the class labels to the train_target array\n",
    "rotated_target = np.append(rotated_target,rotated_target)\n",
    "print(np.shape(rotated_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73328, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "rotations_array = np.empty((np.shape(rotated_images)[0],28,28))\n",
    "for i in range(np.shape(processed_data)[0]):\n",
    "    degree = np.random.uniform(-180,180,1)\n",
    "    img = Image.fromarray(processed_data[i])\n",
    "    rot = img.rotate(degree)\n",
    "    rot = np.asarray(rot)\n",
    "    rotations_array[i] = rot\n",
    "#now append rotations_array to train_images\n",
    "rotated_images = np.append(rotated_images, rotations_array,0)\n",
    "#now append the class labels to the train_target array\n",
    "rotated_target = np.append(rotated_target,rotated_target)\n",
    "print(np.shape(rotated_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrotations_array = np.empty((np.shape(rotated_images)[0],28,28))\\nfor i in range(np.shape(processed_data)[0]):\\n    degree = np.random.uniform(-180,180,1)\\n    img = Image.fromarray(processed_data[i])\\n    rot = img.rotate(degree)\\n    rot = np.asarray(rot)\\n    rotations_array[i] = rot\\n#now append rotations_array to train_images\\nrotated_images = np.append(rotated_images, rotations_array,0)\\n#now append the class labels to the train_target array\\nrotated_target = np.append(rotated_target,rotated_target)\\nprint(np.shape(rotated_images))\\nprint(np.shape(rotated_target))\\n'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "rotations_array = np.empty((np.shape(rotated_images)[0],28,28))\n",
    "for i in range(np.shape(processed_data)[0]):\n",
    "    degree = np.random.uniform(-180,180,1)\n",
    "    img = Image.fromarray(processed_data[i])\n",
    "    rot = img.rotate(degree)\n",
    "    rot = np.asarray(rot)\n",
    "    rotations_array[i] = rot\n",
    "#now append rotations_array to train_images\n",
    "rotated_images = np.append(rotated_images, rotations_array,0)\n",
    "#now append the class labels to the train_target array\n",
    "rotated_target = np.append(rotated_target,rotated_target)\n",
    "print(np.shape(rotated_images))\n",
    "print(np.shape(rotated_target))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_indice = int(np.round(test_train_split*np.size(rotated_target)))\n",
    "\n",
    "\n",
    "#split my data between training and test sets\n",
    "train_target = rotated_target[0:train_split_indice]\n",
    "test_target = rotated_target[train_split_indice::]\n",
    "train_images = rotated_images[0:train_split_indice]\n",
    "test_images = rotated_images[train_split_indice::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to form a subset of our training examples that are probabilities that they are sorted correctly greater than %90\n",
    "#we will then train this dataset additional times.\n",
    "\"\"\"\n",
    "confidence = list()\n",
    "HP_indice = list()\n",
    "for i in range(len(train_images)):\n",
    "    if train_target[i] == 0:\n",
    "        confidence.append(Spiral[i])\n",
    "    if train_target[i] == 1:\n",
    "        confidence.append(Elliptical[i])\n",
    "    if train_target[i] == 2:\n",
    "        confidence.append(Anythingelse[i])\n",
    "    if confidence[i] >= HP_allowed_confidence:\n",
    "        HP_indice.append(i)\n",
    "train_weights = np.asarray(confidence)\n",
    "HP_target = np.empty(len(HP_indice))\n",
    "HP_images = np.empty((len(HP_indice),28,28))\n",
    "for i in range(len(HP_indice)):\n",
    "    HP_target[i] = train_target[HP_indice[i]]\n",
    "    HP_images[i] = train_images[HP_indice[i]]\n",
    "\n",
    "#print(len(HP_indice))\n",
    "HP_images = HP_images.reshape(len(HP_indice),28,28,1)\n",
    "train_images = train_images.reshape(train_split_indice,28,28,1)\n",
    "test_images = test_images.reshape(train_split_indice-number,28,28,1)\n",
    "print(np.shape(train_images))\n",
    "print(np.shape(HP_images))\n",
    "\"\"\"\n",
    "\n",
    "#I need this bottom part to come out, but delete it if we wanted the HP set again...\n",
    "train_images = train_images.reshape(train_split_indice,28,28,1)\n",
    "test_images = test_images.reshape(np.shape(test_images)[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\\n\\ndatagen = ImageDataGenerator(\\n        rotation_range=45,\\n        #width_shift_range=0.2,\\n        #height_shift_range=0.2,\\n        #zoom_range=0.2,\\n        horizontal_flip=True,\\n        vertical_flip=True,\\n        fill_mode='nearest')\\n\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next Idea: I want to augment my images to artificially create more data: technically, I should be allowed to do any\n",
    "#amount of flips and rotations because of the cosmological principle\n",
    "\n",
    "#if that doesn't work, I will just have to get more data. or add more layers...\n",
    "\n",
    "#Note: using this generator decreased performance, I dont know how it works. I do know how my manuel offline flips and rotations\n",
    "#work. And, those offline incresed performance.\n",
    "\n",
    "\"\"\"\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=45,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define my model, using a CNN with 2 Convolutional layers, 2 max pool layers, 1 dense layer, 1 drop out layer, and another dense layer \n",
    "def create_model(dropout_rate=dropout_rate, learning_rate=learning_rate):\n",
    "    \n",
    "    model = keras.Sequential([])\n",
    "    model.add(keras.layers.Conv2D(input_shape=(28,28,1),filters=CNL1_filters,kernel_size=CNL1_kernal_size,padding=\"same\",activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=MPL1_pool_size, strides=MPL1_strides))\n",
    "    model.add(keras.layers.Conv2D(filters=CNL2_filters,kernel_size=CNL2_kernal_size,padding=\"same\",activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=MPL2_pool_size,strides=MPL2_strides))\n",
    "    model.add(keras.layers.Reshape([7*7*64]))\n",
    "    model.add(keras.layers.Dense(units=1024,activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(keras.layers.Dense(units=3,activation=tf.nn.softmax))\n",
    "    adam = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    return(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model with the high probability first, 2-3 epochs increased accuracy just slightly.\n",
    "#model=create_model(dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "#model.fit(HP_images, HP_target, epochs=2, batch_size=batch_size, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "#test_loss, test_acc = model.evaluate(test_images.reshape(number - train_split_indice,28,28,1), test_target)\n",
    "#print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "43008/62329 [===================>..........] - ETA: 1:09 - loss: 0.7013 - acc: 0.6600"
     ]
    }
   ],
   "source": [
    "#lets try training one epoch at a time and seeing what epoch when we maximize our accuracy; I fear overtraining\n",
    "\n",
    "#OTHER than that, my next idea is to change the architecture.... eventually I will need to build a RESnet anyways\n",
    "model = create_model(dropout_rate=dropout_rate, learning_rate = learning_rate)\n",
    "\n",
    "for i in range(epoch_number):\n",
    "    model.fit(train_images, train_target, epochs=1, batch_size=batch_size, verbose=1, shuffle=True)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_target)\n",
    "    print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_model(dropout_rate=dropout_rate, learning_rate = learning_rate)\n",
    "#model.fit(train_images, train_target, epochs=1, batch_size=batch_size, verbose=1, shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "test_loss, test_acc = model.evaluate(test_images, test_target)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "#0.848 with the zeros fixed\n",
    "#0.850 with the HP dataset implemented\n",
    "\n",
    "#0.822 with augmentation, rescaling\n",
    "#rescaling seems to worsen my performance; however: why are some of my pixel values negative?\n",
    "\n",
    "#0.842 with 50 epochs: doesn't seem to be over fitting, perhaps the data augmentation just takes more time to train???\n",
    "\n",
    "#0.852 with 75 epochs, maybe we need to decrease learning rate past 50???, using generator\n",
    "\n",
    "#0.857 w/ 75 epochs, using generator\n",
    "\n",
    "#0.864 with 20 epochs, using offline flips before the data set. dropout = 0.3, no HP set. perhaps we can train for 15 epochs with\n",
    "#LR  0.001 as usual, then train 5 epochs with LR decreased by factor of 10? \n",
    "\n",
    "#I dont think changing my LR will help, it appears Adam already adapts LR on the fly for the situation.\n",
    "#0.857 w/ droput rate increased to 0.3, will decrease it again... \n",
    "\n",
    "#0.864 with 15 epochs, dropout_rate = 0.25, with 124656 images after making two copies of flips and stuff.\n",
    "\n",
    "#0.852 with 20 epcohs, dropout rate = 0.5, LR = 0.004, batch_size = 128. The accuracy did not seem over trained by the end of the \n",
    "#20 epochs, therefore I will run it through another 20 to see what happens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
