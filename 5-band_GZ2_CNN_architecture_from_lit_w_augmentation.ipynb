{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "import os\n",
    "import skimage\n",
    "import random\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(64,64), n_channels=5,\n",
    "                 n_classes=2, shuffle=True):\n",
    "     #   'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "    #'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "    #'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "    # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "      # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "          # Store sample\n",
    "            X[i,] = np.load('image_arrays/' + ID + '.npy')\n",
    "            #flip\n",
    "            if random.random() > 0.5:\n",
    "                X[i,] = np.flip(X[i,],0)\n",
    "            if random.random() > 0.5:\n",
    "                X[i,] = np.flip(X[i,],1)\n",
    "            #rotate\n",
    "            angle = 360*random.random()\n",
    "            X[i,] = skimage.transform.rotate(X[i,], angle=angle, mode='reflect')\n",
    "          # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "            \n",
    "        return X, y\n",
    "        #return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "    #'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    #  'Generate one batch of data'\n",
    "      # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "      # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "      # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCNL1_filters = 32\\nCNL1_kernal_size = 5\\nMPL1_pool_size= (2,2)\\nMPL1_strides = 2\\nCNL2_filters = 64\\nCNL2_kernal_size = 5\\nMPL2_pool_size = (2,2)\\nMPL2_strides = 2\\n'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input variables\n",
    "path = 'image_arrays_new\\\\'\n",
    "validation_path = path + 'validation'\n",
    "training_path = path + 'training'\n",
    "test_path = path + 'test'\n",
    "#model variables\n",
    "batch_size = 32 #32 == best, run 156079264\n",
    "epoch_number = 50\n",
    "learning_rate = 1e-3 #1e-4 stable, overtraining started after step 30, #1e-3 best, OT @ epoch 15\n",
    "\n",
    "params = {'dim': (64,64),\n",
    "          'batch_size': batch_size,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 5,\n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "#architecture variables\n",
    "\"\"\"\n",
    "CNL1_filters = 32\n",
    "CNL1_kernal_size = 5\n",
    "MPL1_pool_size= (2,2)\n",
    "MPL1_strides = 2\n",
    "CNL2_filters = 64\n",
    "CNL2_kernal_size = 5\n",
    "MPL2_pool_size = (2,2)\n",
    "MPL2_strides = 2\n",
    "\"\"\"\n",
    "#defined within because were modeling after a paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxyzoo = pd.read_csv(\"zoo2MainSpecz.csv\", usecols=[8], nrows=10000)\n",
    "Class = galaxyzoo[\"gz2class\"].values\n",
    "dictionary = {'A':int(2),'E':np.array([0]),'S':np.array([1])}\n",
    "#resave using my dictionary\n",
    "target = np.empty((len(Class)))\n",
    "for i in range(len(Class)):\n",
    "    target[i] = dictionary[Class[i][0]]\n",
    "#target = target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = os.listdir(training_path)\n",
    "for i,file in enumerate(train_list):\n",
    "    train_list[i] = file.split('.')[0]\n",
    "val_list = os.listdir(validation_path)\n",
    "for i,file in enumerate(val_list):\n",
    "    val_list[i] = file.split('.')[0]\n",
    "\n",
    "partition = {'train':train_list,'validation':val_list}\n",
    "\n",
    "labels = {}\n",
    "for i in range(10000):\n",
    "    name = 'array_number_{}'.format(i)\n",
    "    labels.update({name:target[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndirectory = os.listdir('image_arrays')\\ndata = np.zeros((len(directory),64,64,5))\\ninbetween_target = np.zeros((len(directory)))\\nfor i in range(len(directory)):\\n    ith = directory[i].split('_')[-1]\\n    ith = ith.split('.')[0]\\n    ith = int(ith)\\n    inbetween_target[i] = target[ith]\\ntarget = inbetween_target\\n\""
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "directory = os.listdir('image_arrays')\n",
    "data = np.zeros((len(directory),64,64,5))\n",
    "inbetween_target = np.zeros((len(directory)))\n",
    "for i in range(len(directory)):\n",
    "    ith = directory[i].split('_')[-1]\n",
    "    ith = ith.split('.')[0]\n",
    "    ith = int(ith)\n",
    "    inbetween_target[i] = target[ith]\n",
    "target = inbetween_target\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    learning_rate = 0.2\\n    if epoch > 10:\\n        learning_rate = 0.02\\n    if epoch > 20:\\n        learning_rate = 0.01\\n    if epoch > 50:\\n        learning_rate = 0.005\\n\\n    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\\n    return learning_rate\\n\\n    lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\\n    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\\n\""
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is pretty neat, you can create a keras callback to display on tensorboard using a simplified summary tf api\n",
    "\n",
    "#and also this is an example of how to change the lr on the fly, which is pretty handy\n",
    "#https://keras.io/callbacks/\n",
    "\n",
    "logdir=\"summaries/scalars/\" + str(datetime.datetime.now().timestamp())\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir,\n",
    "                                                   histogram_freq=1,\n",
    "                                                   write_graph=False,\n",
    "                                                   write_grads=True,)\n",
    "                                                   #write_images=True)\n",
    "\"\"\"\n",
    "    file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "    file_writer.set_as_default()\n",
    "\n",
    "    def lr_schedule(epoch):\n",
    "\"\"\"\n",
    "#Returns a custom learning rate that decreases as epochs progress.\n",
    "\"\"\"\n",
    "    learning_rate = 0.2\n",
    "    if epoch > 10:\n",
    "        learning_rate = 0.02\n",
    "    if epoch > 20:\n",
    "        learning_rate = 0.01\n",
    "    if epoch > 50:\n",
    "        learning_rate = 0.005\n",
    "\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "    lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\"\"\"\n",
    "#will it still print stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate=learning_rate):\n",
    "    \n",
    "    model = keras.Sequential([])\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(input_shape=(64,64,5),filters=32,kernel_size=6,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=64,kernel_size=5,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2,))\n",
    "    model.add(keras.layers.Dropout(rate=0.25)) #best = 0.25\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=128,kernel_size=2,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2,))\n",
    "    model.add(keras.layers.Dropout(rate=0.25)) #best = 0.25\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.25)) #best = 0.35\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=64,activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(rate=0.5))\n",
    "    model.add(keras.layers.Dense(units=2,activation=tf.nn.softmax))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='sparse_categorical_crossentropy',metrics=['accuracy',])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model = create_model(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        5792      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                2097216   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,334,882\n",
      "Trainable params: 2,334,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-117:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 619, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 600, in pool_fn\n",
      "    workers, initializer=init_pool_generator, initargs=(seqs, None))\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    context=self.get_context())\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\", line 176, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\awe2\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: can't pickle generator objects\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "steps_to_take = int(len(os.listdir(validation_path))/batch_size)\n",
    "val_steps_to_take = int(len(os.listdir(training_path))/batch_size)\n",
    "                #typically be equal to the number of unique samples if your dataset\n",
    "                #divided by the batch size.\n",
    "\n",
    "hist = model.fit_generator(generator=training_generator,\n",
    "                    steps_per_epoch=steps_to_take, \n",
    "                    epochs=epoch_number,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=val_steps_to_take,\n",
    "                    verbose=2,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loss, test_acc = model.evaluate(test_images, test_target)\n",
    "#print('Test accuracy:', test_acc)\n",
    "#print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all above are models using a different architecture. all below are comments from the current architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source list\n",
    "\"\"\"\n",
    "https://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/\n",
    "\n",
    "https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "\n",
    "https://astrobites.org/2018/07/16/creating-a-more-general-deep-learning-algorithm-for-galaxies/\n",
    "\n",
    "https://github.com/jameslawlor/kaggle_galaxy_zoo/blob/master/galaxy_zoo_keras.ipynb\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
